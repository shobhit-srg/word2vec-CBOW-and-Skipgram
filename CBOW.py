# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aklSwKQIGed3tabopRSJLVHgCl6634eo

# CBOW Model
"""

from google.colab import drive
drive.mount('/content/gdrive')
root_path = 'gdrive/My Drive/nlp/'

"""Reading Sentences from JSON File and splitting to tokens"""

f = open(root_path+"reviews_Electronics_5.json",)
u=50000
sentences=""
while u>0:
  line= f.readline()
  dic=eval(line)
  sentences+=dic['reviewText'].lower()

  u=u-1
# print(sentences)
sentences=sentences.split()
# print(sentences)

"""Removing Numericals and stopwords and blank tokens"""

import string
from collections import defaultdict 
import numpy as np
from spacy.lang.en.stop_words import STOP_WORDS
table = str.maketrans('', '', string.punctuation)
corpus = [w.translate(table) for w in sentences]
def hasNumbers(inputString):
  return any(char.isdigit() for char in inputString)
for i in corpus:
  if hasNumbers(i):
    corpus.remove(i)
  elif i in STOP_WORDS:
    corpus.remove(i)
  elif i == "":
    corpus.remove(i)
# print(corpus)
print("Total words: {}".format(len(corpus)))
print("Unique words: {}".format(len(set(corpus))))

"""# Subsampling"""

c=defaultdict(int)
for i in corpus:
  c[i] +=1

def subsampling(corpus):
  new_corpus=[]
  for w in corpus:
    cnt=c[w]
    freq=cnt/len(corpus)
    prob = (np.sqrt(freq/0.001) + 1) * (0.001/freq)
    if np.random.random() < prob:   #generates random number (0,1)
      new_corpus.append(w)
  return new_corpus

corpus=subsampling(corpus)
# print(corpus)
print("Total words: {}".format(len(corpus)))
print("Unique words: {}".format(len(set(corpus))))

c3=defaultdict(int)
for i in corpus:
  c3[i] +=1
print(c3)

"""Removing Less Frequent words"""

for i in corpus:
  if c[i] < 8:
    corpus.remove(i)

c=defaultdict(int)
for i in corpus:
  c[i] +=1
# print(c)
c1=sorted(c.keys())
vocab = defaultdict(int)
for i in range(len(c1)): 
  vocab[c1[i]]=i
print("Total words: {}".format(len(corpus)))
print("Unique words: {}".format(len(set(corpus))))
V=len(vocab)
# print(vocab)

"""# One Hot encoding of context and centerwords"""

x_train=[]
y_train=[]
window_size=2
for i in range(len(corpus)):
  w1=[0 for x in range(V)]
  w1[vocab[corpus[i]]]=1
  ww=[]
  for j in range(i-window_size,i+window_size+1):
    if i!=j and j>=0 and j<len(corpus):
      w2=[0 for x in range(V)]
      w2[vocab[corpus[j]]]=1
      ww.append(w2)
  x_train.append(w1)
  y_train.append(ww)

# print(len(y_train[4]))
# print(len(y_train[4][1]))
# print(np.mean(y_train[4], axis=0))

"""# Softmax Formula"""

def softmax(x):
	e_x = np.exp(x - np.max(x)) 
	return e_x / e_x.sum()

"""# My CBOW network and training Data"""

epochs=4
N=20
alpha=0.005
W = np.random.uniform(-0.8, 0.8, (V, N))
W1 = np.random.uniform(-0.8, 0.8, (N, V))
# print(W.shape)

for epoch in range(1,epochs):
  loss=0
  for j in range(len(x_train)):
    if(j%4000==0):
      print(j)
    #forward
    x = np.mean(y_train[j], axis=0)
    h = np.dot(W.T, x)
    # print(h.shape)
    u = np.dot(W1.T, h)
    # print(u.shape)
    y_pred = softmax(u)
    # print(y_pred.shape)
    #backprop
    e = y_pred - x_train[j]
    dW1 = np.outer(h, e)
    dW = np.outer(x, np.dot(W1, e))
    W = W - alpha * dW
    W1 = W1 - alpha * dW1
    for m in x_train[j]:
      if(x_train[j][m]):
        loss += -float(u[m])
    loss += np.log(np.sum(np.exp(u)))
  print("epoch ",epoch, " loss = ",loss)

"""# Context Closest Neighbours and TSNE"""

neighbours=10
wrd=["home","gps","width","i"]
wrd_v=[]
target_vectors=[]
word_vector=[]
# if wrd in corpus:
X = [0 for i in range(V)] 
for hj in wrd:
  X[vocab[hj]] = 1
# X=Xlen(wrd)
X = np.divide(X, len(wrd)) 
h = np.dot(W.T,X).reshape(N,1) 
wrd_v=h
u = np.dot(W1.T,h) 
y = softmax(u)
output = {} 
for i in range(V): 
  output[y[i][0]] = i 
# print(output)
# print(sorted(output,reverse=True))
top_context_words = [] 
for k in sorted(output,reverse=True):
  # print("k",k)
  # print(output[k])
  wrd2=corpus[output[k]]
  # print(wrd2)
  y = [0 for i in range(V)] 
  y[vocab[wrd2]] = 1
  h1 = np.dot(W.T,y).reshape(N,1)
  target_vectors.append(h1)
  top_context_words.append(corpus[output[k]]) 
  if(len(top_context_words)>=neighbours): 
    break
print("The words closest to set of words:")
print(top_context_words) 
# else: 
  # print("Word not found in dicitonary") 

from numpy import dot
from numpy.linalg import norm

def cos(a,b):
  return dot(a, b)/(norm(a)*norm(b))

def flat(l):
  flat_list = []
  for sublist in l:
    for item in sublist:
      flat_list.append(item)
  return flat_list

"""# TSNE"""

import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

tsne = TSNE()
tv=[]
for i in target_vectors:
  tv.append(flat(i))
embed_tsne = tsne.fit_transform(tv)

fig, ax = plt.subplots(figsize=(5, 5))
for idx in range(len(target_vectors)):
    plt.scatter(*embed_tsne[idx, :], color='steelblue')
    plt.annotate(top_context_words[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)

"""# Gensim Comparison"""

from gensim.models import Word2Vec
import gensim.downloader as api
corpus = api.load('text8')
model = Word2Vec(corpus)

gensim_vec= model.wv[wrd[0]]
for i in range(1,len(wrd)):
  # print(wrd[i])
  gensim_vec = np.add(gensim_vec,  model.wv[wrd[i]])  

print("The Mean Error between Gensim Embedding and My Embedding for The word",wrd)
print((np.square(gensim_vec - wrd_v)).mean())